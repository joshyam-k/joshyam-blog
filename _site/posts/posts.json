[
  {
    "path": "posts/2021-12-06-post12-6/",
    "title": "ggplot, geometry, and visualizing probability",
    "description": "using ggplot to solve a problem that seemed hopelessly confusing at first",
    "author": [
      {
        "name": "Josh Yamamoto",
        "url": "https://joshyam-blog.netlify.app"
      }
    ],
    "date": "2021-12-06",
    "categories": [],
    "contents": "\nI was recently given the following problems in one of my math classes\n\nLet \\(X\\) and \\(Y\\) be i.i.d \\(\\text{Unif}(0,1)\\) and define a variable \\(Z = ⌊X/Y⌋\\), where \\(⌊z⌋\\) denotes the greatest integer less than or equal to \\(z\\)\nFind the probability that Z is an odd number\n\nat first glance I had absolutely no idea how to solve this problem. And even as I started to understand what the question was asking I still didn’t really know how one would go about solving it.\nI knew that the problem could be rephrased as\nFind\n\\[P(\\text{Z odd}) = P(Z = 1) + P(Z = 3) + P(Z = 5) \\ +  \\ ...\\]\nSo I figured I might as well just try to solve for these first few probabilities and see if I could find a pattern.\nIf you think about it, if \\(X,Y\\) are i.i.d \\(\\text{Unif}(0,1)\\) then we can imagine each realization of X and Y being like the x and y coordinates of a random point in the cartesian product \\([0,1] \\times [0,1]\\). And if we want to find the probability that \\(Z = 1\\) then we can just generate many such coordinates and then see how many of these coordinates satisfy our condition. This is a lot of words and it might still be confusing so lets make a plot. We’ll write it as a function since we’ll want to make more of these plots later on:\n\n\nlibrary(tidyverse)\ndata_points <- tibble(\n  x = runif(15000, 0, 1),\n  y = runif(15000, 0, 1)\n)\n\nfloor_plot <- function(data, z) {\n  data %>% \n    ggplot(aes(x, y , color  = (floor(x/y) == {{ z }}))) +\n    geom_point(size = 1.5) +\n    scale_color_manual(values = c(\"midnightblue\", \"cyan4\")) +\n    theme_minimal()\n}\n\nfloor_plot(data_points, 1)\n\n\n\n\nand so since we know that the area of the entire square is just one, then the probability that \\(Z = 1\\) can be approximated by counting what fraction of the 10,000 points that we generated is in that cyan streak. Moreover we can imagine generating infinitely many points and coloring them accordingly, and in a monte carlo fashion, say that the probability that \\(Z = 1\\) is just the area of that cyan section. And once you get to see it visually it makes sense why this is the case.\nIn order to have \\(\\lfloor X/Y \\rfloor =1\\) then we certainly need \\(X > Y\\), since otherwise the floor is zero, but if we make X too big then we end up with 2 or 3 and so on. So we require that \\(X \\ge Y\\) but also that \\(X < 2Y\\). In other words, once X gets to be twice as large as Y, then the floor is larger than 1.\nAnd if we want to find \\(P(Z = 3)\\)?\n\n\nfloor_plot(data_points, 3)\n\n\n\n\nand \\(P(Z = 5)\\)?\n\n\nfloor_plot(data_points, 5)\n\n\n\n\nand how about all odd numbers? (which is what we’re really trying to find)\n\n\ndata_points %>% \n  ggplot(aes(x, y, color = (floor(x/y)%%2 == 1))) +\n  geom_point(size = 1.3) +\n  scale_color_manual(values = c(\"midnightblue\", \"cyan4\")) +\n  theme_minimal()\n\n\n\n\ncool right?\nA problem that seemed totally opaque is actually pretty intuitive when visualized.\nNow for the math geeks out there we can actually solve for this probability by writing it as a sum. Recall how we said that the conditions for \\(P(Z = 1)\\) were that \\(Y \\le X < 2Y\\), well similarly, the conditions for \\(P(Z = 3)\\) are that \\(3Y \\le X < 4Y\\) and the pattern continues.\nSo we can write the sum of all these triangles as\n\\[P(\\text{Z odd}) = \\sum_{i=1}^\\infty\\frac{1}{2}\\bigg(\\frac{1}{2i-1} - \\frac{1}{2i}\\bigg) = \\frac{\\ln(2)}{2} \\approx 0.34\\]\nwhich ends up being a very nice answer! thanks ggplot\n\n\n\n",
    "preview": "posts/2021-12-06-post12-6/post12-6_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-12-06T22:50:39-08:00",
    "input_file": "post12-6.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Spatial interpolation using polynomial regression in R",
    "description": "Using 5-fold CV to determine the best polynomial degree for modeling precipitation data\nin California.",
    "author": [
      {
        "name": "Josh Yamamoto",
        "url": "https://joshyam-blog.netlify.app"
      }
    ],
    "date": "2021-03-14",
    "categories": [],
    "contents": "\nIn this post, I’ll walk through using polynomial regression for spatial interpolation using Precipitation data from California.\nData Prep and Motivation\nWe’ll start by loading in all of the necessary packages.\n\n\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(rspatial)\nlibrary(dismo)\nlibrary(sf)\n\n\n\nNext we’ll load in the data that we’ll need, and then convert it to a SpatialPointsDataFrame object.\n\n\nrain_data_raw <- sp_data('precipitation')\nrain_data_raw$prec <- rowSums(rain_data_raw[, c(6:17)])\nCA <- sp_data(\"counties\")\n\nrain_sp <- SpatialPoints(rain_data_raw[,4:3], proj4string=CRS(\"+proj=longlat +datum=NAD83\"))\nrain_sp <- SpatialPointsDataFrame(rain_sp, rain_data_raw)\n\n\n\nWe’d like to use a specific coordinate reference system since we’re only looking at California, and so we’ll specify it and then apply it to our data. Don’t worry about the cata and ca lines, they’re just creating a california boundary object for us, and we wont use them until the very end.\n\n\nnew_crs <- CRS(\"+proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 +y_0=-4000000\n               +datum=NAD83 +units=m +ellps=GRS80 +towgs84=0,0,0\")\n\nrain_new <- spTransform(rain_sp, new_crs)\n\ncata <- spTransform(CA, new_crs)\nca <- aggregate(cata)\n\n\n\nFinally for modeling we’d like our precipitation data to be a data frame with coordinates and precipitation levels.\n\n\nrain <- as.data.frame(rain_new$prec)\ncolnames(rain) <- c(\"prec\")\n\nrain$X <- coordinates(rain_new)[,1]\nrain$Y <- coordinates(rain_new)[,2]\n\n\n\nSo what’s the end goal here, and what is spatial interpolation?\nWell we have precipitation data for individual points across California:\n\n\nca_sf <- st_as_sf(ca)\n\nggplot() +\n  geom_sf(data = ca_sf) + \n  geom_point(data = rain, aes(x = X, y = Y, color = prec)) +\n  coord_sf() +\n  theme_void()\n\n\n\n\nBut what if we’d like to predict precipitation levels for every single location in California? One (not very good) option is to fit a polynomial regression to the existing points and then use that model to predict the precipitation levels for the entire field.\nSupplementary functions\nWe’ll evaluate our model using the residual mean squared error or RMSE, so we’ll write a quick function that can compute that for us:\n\n\nRMSE <- function(observed, predicted) {\n  sqrt(mean((predicted - observed)^2, na.rm=TRUE))\n}\n\n\n\nAdditionally we’ll be trying varying degrees for our polynomial regression so it’ll be nice to have a function that can make that process simpler for us. For this exercise in spatial interpolation we’re going to be using every interaction term and since that requires an \\(n - 1\\) argument we have to specify the special case for when \\(n = 1\\). Notice how specialized to this individual exercise this function is. We know that the data we feed in will have a “prec” column and that X and Y will always be in our dataset.\n\n\npoly_mod <- function(n, data){\n  if (n == 1){\n    lm(data$prec ~ X + Y, data)\n  } else {\n    lm(data$prec ~ poly(X, n) + poly(Y,n) + poly(X, n - 1):poly(Y, n - 1), data)\n  }\n}\n\n\n\nA 5-fold CV function specific to this data\nSo now we have all the tools to write a function that will perform cross validation for us. The function looks a little confusing but it’s just doing the following 5 simple steps\nspecify which indices belong to which fold\ncreate an empty vector to be filled with rmse values\nfor each fold fit the model to the training data, and then apply the model to the test data\ncalculate the rmse and add it to our empty vector\nafter all folds have been used, take the output mean of the 5 rmse values\nNotice our function has just one argument, n and this specifies the degree of our polynomial.\n\n\nk_fold_func <- function(n){\n  \n  folds <- kfold(nrow(rain_new))\n  rmse <- rep(NA, 5)\n  \n  for (i in 1:5) {\n    test <- rain[folds == i, ]\n    train <- rain[folds != i, ]\n    mod <- poly_mod(n, data = train)\n    preds <- predict(mod, newdata = test)\n  \n    rmse[i] <- RMSE(preds, test$prec)\n  }\n  mean(rmse)\n}\n\n\n\nPutting our functions to use!\nNow we can finally fit some models and use our cross-validation function. We’ll try a range from 1-10 for our different polynomial degrees. It’s important to set a seed here because our folds are randomly created. We’ll use purrr::map to apply our function to our range of n’s\n\n\nset.seed(27)\nrange <- 1:10\n\nrmse_vals <- range %>% \n  purrr::map(k_fold_func)\n\n\n\nWe’ll find the rmse of a null model that we can compare our polynomial regression to:\n\n\nnull <- RMSE(mean(rain_sp$prec), rain_sp$prec)\n\n\n\nAnd now we’ll turn rmse_vals into a data frame and then visualize it:\n\n\nrmse_df <- as.data.frame(do.call(rbind, rmse_vals)) \n\ncolnames(rmse_df) <- c(\"rmse\")\n\nrmse_df$n <- 1:10\n\nrmse_df %>% \n  ggplot(aes(x = factor(n), y = rmse, group = 1)) +\n  geom_point(size = 3, color = \"cyan4\") +\n  geom_line() +\n  geom_hline(yintercept = null, size = 1.5, alpha = 0.6, color = \"midnightblue\") +\n  annotate(\"text\", x = 1.5, y = 470, label = \"Null\") +\n  theme_minimal() +\n  labs(\n    x = \"Polynomial Degree\",\n    y = \"RMSE\"\n  )\n\n\n\n\nIt looks like we hit a sweet spot right around \\(n = 5\\) and it looks like anything above \\(n = 7\\) is when we start to severely over-fit. So now we’ll use our polynomial of degree 5 to perform spatial interpolation for California Precipitation.\nInterpolation\nEssentially all we’re doing here is fitting our model, creating a grid object across which we can make predictions, and then adding those predictions to the grid.\n\n\nbest_mod <- poly_mod(5, rain)\n\ngrid <- as.data.frame(spsample(rain_new, \"regular\", n = 50000))\nnames(grid) <- c(\"X\", \"Y\")\ncoordinates(grid) <- c(\"X\", \"Y\")\ngridded(grid) <- TRUE\nfullgrid(grid) <- TRUE \n\npredictions <- SpatialGridDataFrame(grid, data.frame(var1.pred = predict(best_mod, newdata = grid))) \n\n\n\nFinally we’ll turn our grid into a raster and trim it so that it is contained inside of California’s boundaries and plot it!\n\n\nraster_preds <- raster(predictions)\ntrimmed_raster <- mask(raster_preds, ca)\n\nplot(trimmed_raster)\n\n\n\n\nWhen we compare this to our original plot of just points we can see that it follows similar trends, with higher precipitation levels being characteristic of northern California. Furthermore since our 5th degree polynomial substantially beats out the null model, there is also some merit in this approach, but ultimately methods like proximity polygons and inverse distance weighting will likely perform better.\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-14T13:39:25-07:00",
    "input_file": {}
  }
]
